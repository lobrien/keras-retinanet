{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1fd9570466736b09c16d4ed020cb81aef494753d"
   },
   "source": [
    "# **ESRI DATA SCIENCE CHALLENGE 2019**\n",
    "\n",
    "\n",
    "In this challenge, we have been given 224x224 images containing cars and swimming pools labelled in PASCAL VOC Format. As we'll be using RetinaNet for this challenge, we'll make use of keras-retinanet package from github. \n",
    "* [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002) - Research paper describing RetinaNet and Focal Loss which it uses\n",
    "* [keras-retinanet](https://github.com/fizyr/keras-retinanet) - Keras RetinaNet implimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Users/larryobrien/.local/lib/python3.6/site-packages (3.4.3.18)\n",
      "Requirement already satisfied: imgaug in /Users/larryobrien/anaconda3/lib/python3.6/site-packages (0.2.9)\n",
      "Requirement already satisfied: keras-retinanet in /Users/larryobrien/.local/lib/python3.6/site-packages (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/larryobrien/anaconda3/lib/python3.6/site-packages (from opencv-python) (1.16.2)\n",
      "Requirement already satisfied: Pillow in /Users/larryobrien/.local/lib/python3.6/site-packages (from imgaug) (5.2.0)\n",
      "Requirement already satisfied: imageio in /Users/larryobrien/anaconda3/lib/python3.6/site-packages (from imgaug) (2.2.0)\n",
      "Requirement already satisfied: Shapely in /Users/larryobrien/anaconda3/lib/python3.6/site-packages (from imgaug) (1.6.4.post2)\n",
      "Requirement already satisfied: scikit-image>=0.11.0 in /Users/larryobrien/anaconda3/lib/python3.6/site-packages (from imgaug) (0.13.0)\n",
      "Requirement already satisfied: scipy in /Users/larryobrien/anaconda3/lib/python3.6/site-packages (from imgaug) (0.19.1)\n",
      "Requirement already satisfied: matplotlib in /Users/larryobrien/anaconda3/lib/python3.6/site-packages (from imgaug) (3.1.0)\n",
      "Requirement already satisfied: six in /Users/larryobrien/anaconda3/lib/python3.6/site-packages (from imgaug) (1.11.0)\n",
      "Requirement already satisfied: cython in /Users/larryobrien/.local/lib/python3.6/site-packages (from keras-retinanet) (0.28.5)\n",
      "Requirement already satisfied: keras-resnet in /Users/larryobrien/.local/lib/python3.6/site-packages (from keras-retinanet) (0.1.0)\n",
      "Requirement already satisfied: keras in /Users/larryobrien/anaconda3/lib/python3.6/site-packages (from keras-retinanet) (2.2.4)\n",
      "Requirement already satisfied: networkx>=1.8 in /Users/larryobrien/anaconda3/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug) (2.0)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /Users/larryobrien/anaconda3/lib/python3.6/site-packages (from scikit-image>=0.11.0->imgaug) (0.5.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/larryobrien/anaconda3/lib/python3.6/site-packages (from matplotlib->imgaug) (2.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/larryobrien/anaconda3/lib/python3.6/site-packages (from matplotlib->imgaug) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/larryobrien/anaconda3/lib/python3.6/site-packages (from matplotlib->imgaug) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/larryobrien/anaconda3/lib/python3.6/site-packages (from matplotlib->imgaug) (2.8.0)\n",
      "Requirement already satisfied: pyyaml in /Users/larryobrien/anaconda3/lib/python3.6/site-packages (from keras->keras-retinanet) (3.12)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Users/larryobrien/anaconda3/lib/python3.6/site-packages (from keras->keras-retinanet) (1.0.6)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/larryobrien/anaconda3/lib/python3.6/site-packages (from keras->keras-retinanet) (1.0.5)\n",
      "Requirement already satisfied: h5py in /Users/larryobrien/anaconda3/lib/python3.6/site-packages (from keras->keras-retinanet) (2.7.1)\n",
      "Requirement already satisfied: decorator>=4.1.0 in /Users/larryobrien/anaconda3/lib/python3.6/site-packages (from networkx>=1.8->scikit-image>=0.11.0->imgaug) (4.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/larryobrien/anaconda3/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->imgaug) (41.0.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install opencv-python imgaug keras-retinanet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "907236db932c74a1170900b8e6d9cc60bd086206"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from os import listdir, walk\n",
    "from os.path import join\n",
    "from keras_retinanet.bin.train import create_generators,create_models,create_callbacks\n",
    "from keras_retinanet.models import backbone,load_model,convert_model\n",
    "from keras_retinanet.utils.config import read_config_file,parse_anchor_parameters\n",
    "from keras_retinanet.utils.visualization import draw_boxes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "tf.set_random_seed(31) # SEEDS MAKE RESULTS MORE REPRODUCABLE\n",
    "np.random.seed(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Change classes to, at most, 1. Or maybe to 'Trash'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "faef2a3eac3226eb2e8211a9c81f89b9a6a811b3"
   },
   "outputs": [],
   "source": [
    "classes = ['1','2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "56a388a1b5fc53f9ffd6c207a87bae5b8ffbc543"
   },
   "source": [
    "# Load and Convert Annotations\n",
    "\n",
    "Here we load annotations given in PASCAL VOC Format and save them in CSV Format as required by keras-retinanet package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: I don't thing I have my annotations in VOC format, so I think I can kill this and replace with another functions that writes to `out_file` appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7673f02c16e5bb5e3734f23d2a533326b9fba562"
   },
   "source": [
    "def convert_annotation(image_id,filename):\n",
    "    in_file = open('training_data/labels/%s.xml'%(image_id))\n",
    "    out_file = open(filename, 'a')\n",
    "    tree=ET.parse(in_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    if root.iter('object') is not None:\n",
    "        for obj in root.iter('object'):\n",
    "            cls = obj.find('name').text\n",
    "            if cls not in classes:\n",
    "                continue\n",
    "            cls_id = classes.index(cls)\n",
    "            \n",
    "            xmlbox = obj.find('bndbox')\n",
    "            x1 = math.ceil(float(xmlbox.find('xmin').text))\n",
    "            y1 = math.ceil(float(xmlbox.find('ymin').text))\n",
    "            x2 = math.ceil(float(xmlbox.find('xmax').text))\n",
    "            y2 = math.ceil(float(xmlbox.find('ymax').text))\n",
    "            if x1 == x2 or y1 == y2:\n",
    "                continue\n",
    "                \n",
    "            out_file.write(f'training_data/images/{image_id}.jpg,{x1},{y1},{x2},{y2},{cls}\\n')\n",
    "    else:\n",
    "        out_file.write(f'training_data/images/{image_id}.jpg,,,,,\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04461e1dd8304d4224c0ce6fe9a1da1040b8aa9d"
   },
   "source": [
    "# Training and Validation split\n",
    "\n",
    "Normally we would have 10-30% of our images in validation set but as we want best possible score we'll use all our images to train, as we have quite few training images already. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import pandas as pd \n",
    "with open('training_data/images/deduped_annotations.csv') as f : \n",
    "    ls = list(csv.reader(f))[1:]\n",
    "    \n",
    "def to_annotation(l) : \n",
    "    # Note swap of 1st and 2nd Y\n",
    "    return (\"training_data/images/\" + l[0][11:],l[1],l[4],l[3],l[2],1)\n",
    "\n",
    "x_train,x_val = train_test_split([to_annotation(l) for l in ls], test_size=0.2)\n",
    "pd.DataFrame(x_train).to_csv(\"annotations.csv\",header=None,index=None)\n",
    "pd.DataFrame(x_val).to_csv(\"val_annotations.csv\",header=None,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "93124e395a1fdc94926afe1b71179770c84d587b"
   },
   "outputs": [],
   "source": [
    "with open('classes.csv','w') as f:\n",
    "    #f.write('1,0\\n2,1\\n')\n",
    "    f.write('1,0\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "add87f5c791fbe8da4793a384e983235587d1626"
   },
   "source": [
    "# Anchor Parameters\n",
    "\n",
    "1. Anchor parameters are used to decide how anchor boxes will be generated for the model.\n",
    "1. As we're dealing mostly small boxes with can be highly elongated, we'll change ratios and scales to fit our needs.\n",
    "1. test_anchors.ipynb is used to visualize anchors on ground truth boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Well... our anchor boxes will be small, but not sure about the highly elongated. I imagine that might result in tweaks to ratios? Like maybe that specifies expected aspect ratios?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "17398f319e28dd6e38948e2e09dc87481e1966bc"
   },
   "outputs": [],
   "source": [
    "with open('config.ini','w') as f:\n",
    "    f.write('[anchor_parameters]\\nsizes   = 32 64 128 256 512\\nstrides = 8 16 32 64 128\\nratios  = 0.25 0.5 0.75 1 1.5 2 4 6 8 10\\nscales  = 0.5 1 2\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "867d2c56814d761227270459f837b787e616f4c5"
   },
   "source": [
    "# Some Hyperparameters\n",
    "\n",
    "We will rescale our images to 672x672 for better precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: I dislike this. And what is 672 x 672 ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_uuid": "ab8e28dc982006ee8f8800a9021433f0d716fc48"
   },
   "outputs": [],
   "source": [
    "b = backbone('resnet50')\n",
    "training_len = 5408\n",
    "testing_len = 1353\n",
    "\n",
    "class args:\n",
    "    batch_size = 64\n",
    "    config = read_config_file('config.ini')\n",
    "    random_transform = True # Image augmentation\n",
    "    annotations = 'annotations.csv'\n",
    "    val_annotations = 'val_annotations.csv'\n",
    "    classes = 'classes.csv'\n",
    "    image_min_side = 672\n",
    "    image_max_side = 672\n",
    "    dataset_type = 'csv'\n",
    "    tensorboard_dir = ''\n",
    "    evaluation = False\n",
    "    snapshots = True\n",
    "    snapshot_path = \"saved/\"\n",
    "    backbone = 'resnet50'\n",
    "    epochs = 5\n",
    "    steps = training_len//(batch_size)\n",
    "    weighted_average = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "_uuid": "e4551ed97d6ee33f91283f559bbb7d9549d2d1f6"
   },
   "outputs": [],
   "source": [
    "train_gen,valid_gen = create_generators(args,b.preprocess_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "48482aa28034ede1fc5dd69661bbf83163ddb7bd"
   },
   "source": [
    "# Image Augmentation\n",
    "\n",
    "In addition to augmentations already done by keras-retinanet [here](https://github.com/fizyr/keras-retinanet/blob/master/keras_retinanet/bin/train.py#L227) , we'll use a package called imgaug to furthur augment the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_uuid": "c09f817a02a3c385181a216f1c7c1af993efcf09"
   },
   "outputs": [],
   "source": [
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "# Define our sequence of augmentation steps that will be applied to every image.\n",
    "seq = iaa.Sequential(\n",
    "    [\n",
    "        #\n",
    "        # Execute 1 to 9 of the following (less important) augmenters per\n",
    "        # image. Don't execute all of them, as that would often be way too\n",
    "        # strong.\n",
    "        #\n",
    "        iaa.SomeOf((1, 9),\n",
    "            [\n",
    "\n",
    "                        # Blur each image with varying strength using\n",
    "                        # gaussian blur (sigma between 0 and .5),\n",
    "                        # average/uniform blur (kernel size 1x1)\n",
    "                        # median blur (kernel size 1x1).\n",
    "                        iaa.OneOf([\n",
    "                            iaa.GaussianBlur((0,0.5)),\n",
    "                            iaa.AverageBlur(k=(1)),\n",
    "                            iaa.MedianBlur(k=(1)),\n",
    "                        ]),\n",
    "\n",
    "                        # Sharpen each image, overlay the result with the original\n",
    "                        # image using an alpha between 0 (no sharpening) and 1\n",
    "                        # (full sharpening effect).\n",
    "                        iaa.Sharpen(alpha=(0, 0.25), lightness=(0.75, 1.5)),\n",
    "\n",
    "                        # Add gaussian noise to some images.\n",
    "                        # In 50% of these cases, the noise is randomly sampled per\n",
    "                        # channel and pixel.\n",
    "                        # In the other 50% of all cases it is sampled once per\n",
    "                        # pixel (i.e. brightness change).\n",
    "                        iaa.AdditiveGaussianNoise(\n",
    "                            loc=0, scale=(0.0, 0.01*255), per_channel=0.5\n",
    "                        ),\n",
    "\n",
    "                        # Either drop randomly 1 to 10% of all pixels (i.e. set\n",
    "                        # them to black) or drop them on an image with 2-5% percent\n",
    "                        # of the original size, leading to large dropped\n",
    "                        # rectangles.\n",
    "                        iaa.OneOf([\n",
    "                            iaa.Dropout((0.01, 0.1), per_channel=0.5),\n",
    "                            iaa.CoarseDropout(\n",
    "                                (0.03, 0.15), size_percent=(0.02, 0.05),\n",
    "                                per_channel=0.2\n",
    "                            ),\n",
    "                        ]),\n",
    "\n",
    "                        # Add a value of -5 to 5 to each pixel.\n",
    "                        iaa.Add((-5, 5), per_channel=0.5),\n",
    "\n",
    "                        # Change brightness of images (85-115% of original value).\n",
    "                        iaa.Multiply((0.85, 1.15), per_channel=0.5),\n",
    "\n",
    "                        # Improve or worsen the contrast of images.\n",
    "                        iaa.ContrastNormalization((0.75, 1.25), per_channel=0.5),\n",
    "\n",
    "                        # Convert each image to grayscale and then overlay the\n",
    "                        # result with the original with random alpha. I.e. remove\n",
    "                        # colors with varying strengths.\n",
    "                        iaa.Grayscale(alpha=(0.0, 0.25)),\n",
    "\n",
    "                        # In some images distort local areas with varying strength.\n",
    "                        sometimes(iaa.PiecewiseAffine(scale=(0.001, 0.01)))\n",
    "                    ],\n",
    "            # do all of the above augmentations in random order\n",
    "            random_order=True\n",
    "        )\n",
    "    ],\n",
    "    # do all of the above augmentations in random order\n",
    "    random_order=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "_uuid": "e2743e4a77f3b2d2e1c702b0c968e07e7603d2cc"
   },
   "outputs": [],
   "source": [
    "def augment_train_gen(train_gen,visualize=False):\n",
    "    '''\n",
    "    Creates a generator using another generator with applied image augmentation.\n",
    "    Args\n",
    "        train_gen  : keras-retinanet generator object.\n",
    "        visualize  : Boolean; False will convert bounding boxes to their anchor box targets for the model.\n",
    "    '''\n",
    "    imgs = []\n",
    "    boxes = []\n",
    "    targets = []\n",
    "    size = train_gen.size()\n",
    "    idx = 0\n",
    "    while True:\n",
    "        while len(imgs) < args.batch_size:\n",
    "            image       = train_gen.load_image(idx % size)\n",
    "            annotations = train_gen.load_annotations(idx % size)\n",
    "            image,annotations = train_gen.random_transform_group_entry(image,annotations)\n",
    "            imgs.append(image)            \n",
    "            boxes.append(annotations['bboxes'])\n",
    "            targets.append(annotations)\n",
    "            idx += 1\n",
    "        if visualize:\n",
    "            imgs = seq.augment_images(imgs)\n",
    "            im2 = np.array(imgs)\n",
    "            boxes = np.array(boxes)\n",
    "            yield im2,boxes\n",
    "        else:\n",
    "            imgs = seq.augment_images(imgs)\n",
    "            imgs,targets = train_gen.preprocess_group(imgs,targets)\n",
    "            imgs = train_gen.compute_inputs(imgs)\n",
    "            targets = train_gen.compute_targets(imgs,targets)\n",
    "            im2 = np.array(imgs)\n",
    "            yield im2,targets\n",
    "        imgs = []\n",
    "        boxes = []\n",
    "        targets = []\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "755f6bb05e3251d40877ddba74c8f6486678f47c"
   },
   "source": [
    "# Visualize augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bcd7d99112b845fe8b42ca980d3842361db93614"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "skip_batches = 5\n",
    "i = 0\n",
    "\n",
    "for imgs,boxes in augment_train_gen(train_gen,visualize=True):\n",
    "    if i > skip_batches:\n",
    "        fig=plt.figure(figsize=(24,96))\n",
    "        columns = 2\n",
    "        rows = 8\n",
    "        for i in range(1, columns*rows + 1):\n",
    "            draw_boxes(imgs[i], boxes[i], (0, 255, 0), thickness=1)\n",
    "            fig.add_subplot(rows, columns, i)\n",
    "            plt.imshow(cv2.cvtColor(imgs[i],cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "35f45954269581be05dbc6a9601965db625609d8"
   },
   "source": [
    "# More Hyperparameters\n",
    "\n",
    "we'll use learning rate of 0.001 and freeze weights for the backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "_uuid": "de392d655a630d3b73dfa84fe1febbed0f3fdbe2"
   },
   "outputs": [],
   "source": [
    "model, training_model, prediction_model = create_models(\n",
    "            backbone_retinanet=b.retinanet,\n",
    "            num_classes=train_gen.num_classes(),\n",
    "            weights=None,\n",
    "            multi_gpu=False,\n",
    "            freeze_backbone=True,\n",
    "            lr=1e-3,\n",
    "            config=args.config\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "_uuid": "a34e90ba8fec837af9007610241b487b9a6a807c"
   },
   "outputs": [],
   "source": [
    "callbacks = create_callbacks(\n",
    "    model,\n",
    "    training_model,\n",
    "    prediction_model,\n",
    "    valid_gen,\n",
    "    args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33285a3b74dc8580797f86678e080cbfecd7c668"
   },
   "source": [
    "# Download pretrained model\n",
    "\n",
    "We download a pretrained model on COCO dataset and load it's weights, we'll skip loading the weights for the few last layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_uuid": "764a0430cc7c5f5f341bb3ba3bb89b699248481e"
   },
   "outputs": [],
   "source": [
    "#!wget https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "_uuid": "275ddf407b82092a0ec8a514500c0987f918d59b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/larryobrien/anaconda3/envs/azureml/lib/python3.7/site-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer regression_submodel due to mismatch in shape ((3, 3, 256, 120) vs (36, 256, 3, 3)).\n",
      "  weight_values[i].shape))\n",
      "/Users/larryobrien/anaconda3/envs/azureml/lib/python3.7/site-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer regression_submodel due to mismatch in shape ((120,) vs (36,)).\n",
      "  weight_values[i].shape))\n",
      "/Users/larryobrien/anaconda3/envs/azureml/lib/python3.7/site-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer classification_submodel due to mismatch in shape ((3, 3, 256, 30) vs (720, 256, 3, 3)).\n",
      "  weight_values[i].shape))\n",
      "/Users/larryobrien/anaconda3/envs/azureml/lib/python3.7/site-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer classification_submodel due to mismatch in shape ((30,) vs (720,)).\n",
      "  weight_values[i].shape))\n"
     ]
    }
   ],
   "source": [
    "training_model.load_weights('resnet50_coco_best_v2.1.0.h5',skip_mismatch=True,by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f7cd58d21715eb0ec3a9de5b30be8cea85915726"
   },
   "source": [
    "# Train the model\n",
    "\n",
    "We will train for 70 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "06bca3482f679c50640b7396bafb9b291b4f402b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
    "        steps_per_epoch=args.steps,\n",
    "        epochs=args.epochs,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a5b84c37c58444c37e6d08fb6de19b756c03926"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
