{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1fd9570466736b09c16d4ed020cb81aef494753d"
   },
   "source": [
    "# **Training A Custom RetinaNet Model on Azure ML**\n",
    "\n",
    "\n",
    "This template Notebook is a quick way to get a RetinaNet model running on Azure ML. This is a Keras implementation and was developed on a Tensorflow backend.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- An [object classification](https://stackoverflow.com/questions/31750076/what-is-the-difference-between-object-detection-and-object-classification) dataset of at least several hundred examples, with images no larger than 512x512 and annotation targets no smaller than 16x16 pixels\n",
    "- An Azure account with access to GPU-accelerated deep-learning machines, such as an NC6 \n",
    "- Familiarity with Python and Jupyter Notebooks\n",
    "\n",
    "## Setup\n",
    "\n",
    "- The notebook looks for images in the directory `training_data/images/`. The notebook resizes images to 512x512. (You can change this to smaller numbers by editing the `args` class below but be aware that the NC6 runs out of GPU memory with 1024x1024 images.)\n",
    "\t\n",
    "- The notebook looks for the annotation file [training_data/images/master_annotations.csv](training_data/images/master_annotations.csv). This file _must_ contain lines of the form: `relative_path_to_image,x0,y0,x1,y1` where `x0, y0` are _pixel values_ of the upper-left corner and `x1, y1` are the lower-right corner. (IMPORTANT!: Some annotation tools produce _normalized_ location values in the range `[0..1]`, some generate location and (height,width), and some generate center and radius.)\n",
    "\n",
    "- Create a `classes.csv`. If there are `k` classes in your problem, there should be `k` lines in your `classes.csv` file. (Not `k+1`! There is no background class in RetinaNet.) \n",
    "\n",
    "- Configure target box size and stride  `tk` tk discussion tk \n",
    "    \n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "[RetinaNet](https://arxiv.org/abs/1708.02002) is an object classification deep-learning architecture. It is an alternative to [YOLO](https://pjreddie.com/darknet/yolo/) and trades higher accuracy for slower performance (although I believe YoloV3 claims to be almost as accurate). \n",
    "\n",
    "The recent addition of Notebook VMs to Azure ML makes it easier to get your [Jupyter](https://jupyter.org)-based ML project into Azure ML. A Notebook VM is, to my mind, a stepping-stone between the easy-to-understand, hard-to-administer scenario of a VM and the harder-to-understand, easy-to-administer, and highly-flexible scenario of the complete Azure ML offering. \n",
    "\n",
    "This notebook is derived from tk, which in turn is derived from tk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuring the run\n",
    "\n",
    "tk explanation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your array of labels for the `n` classes that you wish to distinguish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "faef2a3eac3226eb2e8211a9c81f89b9a6a811b3"
   },
   "outputs": [],
   "source": [
    "classes = ['1']\n",
    "with open('classes.csv','w') as f:\n",
    "    #f.write('1,0\\n2,1\\n')\n",
    "    f.write('1,0\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "!pip3 install opencv-python imgaug keras-retinanet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "907236db932c74a1170900b8e6d9cc60bd086206"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/data/anaconda/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/data/anaconda/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/data/anaconda/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/data/anaconda/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from os import listdir, walk\n",
    "from os.path import join\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras_retinanet.bin.train import create_generators,create_models,create_callbacks\n",
    "from keras_retinanet.models import backbone,load_model,convert_model\n",
    "from keras_retinanet.utils.config import read_config_file,parse_anchor_parameters\n",
    "from keras_retinanet.utils.visualization import draw_boxes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "tf.set_random_seed(31) # SEEDS MAKE RESULTS MORE REPRODUCABLE\n",
    "np.random.seed(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "56a388a1b5fc53f9ffd6c207a87bae5b8ffbc543"
   },
   "source": [
    "# Load and Convert Annotations\n",
    "\n",
    "Here we load annotations given in PASCAL VOC Format and save them in CSV Format as required by keras-retinanet package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: I don't thing I have my annotations in VOC format, so I think I can kill this and replace with another functions that writes to `out_file` appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7673f02c16e5bb5e3734f23d2a533326b9fba562"
   },
   "source": [
    "def convert_annotation(image_id,filename):\n",
    "    in_file = open('training_data/labels/%s.xml'%(image_id))\n",
    "    out_file = open(filename, 'a')\n",
    "    tree=ET.parse(in_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    if root.iter('object') is not None:\n",
    "        for obj in root.iter('object'):\n",
    "            cls = obj.find('name').text\n",
    "            if cls not in classes:\n",
    "                continue\n",
    "            cls_id = classes.index(cls)\n",
    "            \n",
    "            xmlbox = obj.find('bndbox')\n",
    "            x1 = math.ceil(float(xmlbox.find('xmin').text))\n",
    "            y1 = math.ceil(float(xmlbox.find('ymin').text))\n",
    "            x2 = math.ceil(float(xmlbox.find('xmax').text))\n",
    "            y2 = math.ceil(float(xmlbox.find('ymax').text))\n",
    "            if x1 == x2 or y1 == y2:\n",
    "                continue\n",
    "                \n",
    "            out_file.write(f'training_data/images/{image_id}.jpg,{x1},{y1},{x2},{y2},{cls}\\n')\n",
    "    else:\n",
    "        out_file.write(f'training_data/images/{image_id}.jpg,,,,,\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04461e1dd8304d4224c0ce6fe9a1da1040b8aa9d"
   },
   "source": [
    "# Training and Validation split\n",
    "\n",
    "Normally we would have 10-30% of our images in validation set but as we want best possible score we'll use all our images to train, as we have quite few training images already. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import pandas as pd \n",
    "with open('training_data/images/deduped_annotations.csv') as f : \n",
    "    ls = list(csv.reader(f))[1:]\n",
    "    \n",
    "def to_annotation(l) : \n",
    "    # Note swap of 1st and 2nd Y\n",
    "    return (\"training_data/images/\" + l[0][11:],l[1],l[4],l[3],l[2],1)\n",
    "\n",
    "x_train,x_val = train_test_split([to_annotation(l) for l in ls], test_size=0.2)\n",
    "pd.DataFrame(x_train).to_csv(\"annotations.csv\",header=None,index=None)\n",
    "pd.DataFrame(x_val).to_csv(\"val_annotations.csv\",header=None,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "93124e395a1fdc94926afe1b71179770c84d587b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "add87f5c791fbe8da4793a384e983235587d1626"
   },
   "source": [
    "# Anchor Parameters\n",
    "\n",
    "1. Anchor parameters are used to decide how anchor boxes will be generated for the model.\n",
    "1. As we're dealing mostly small boxes with can be highly elongated, we'll change ratios and scales to fit our needs.\n",
    "1. test_anchors.ipynb is used to visualize anchors on ground truth boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Well... our anchor boxes will be small, but not sure about the highly elongated. I imagine that might result in tweaks to ratios? Like maybe that specifies expected aspect ratios?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "17398f319e28dd6e38948e2e09dc87481e1966bc"
   },
   "outputs": [],
   "source": [
    "with open('config.ini','w') as f:\n",
    "    f.write('[anchor_parameters]\\nsizes   = 32 64 128 256 512\\nstrides = 8 16 32 64 128\\nratios  = 0.25 0.5 0.75 1 1.5 2 4 6 8 10\\nscales  = 0.5 1 2\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "867d2c56814d761227270459f837b787e616f4c5"
   },
   "source": [
    "# Some Hyperparameters\n",
    "\n",
    "We will rescale our images to 672x672 for better precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: I dislike this. And what is 672 x 672 ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "ab8e28dc982006ee8f8800a9021433f0d716fc48"
   },
   "outputs": [],
   "source": [
    "b = backbone('resnet50')\n",
    "training_len = 5408\n",
    "testing_len = 1353\n",
    "\n",
    "class args:\n",
    "    batch_size = 64\n",
    "    config = read_config_file('config.ini')\n",
    "    random_transform = True # Image augmentation\n",
    "    annotations = 'annotations.csv'\n",
    "    val_annotations = 'val_annotations.csv'\n",
    "    classes = 'classes.csv'\n",
    "    image_min_side = 512\n",
    "    image_max_side = 512\n",
    "    dataset_type = 'csv'\n",
    "    tensorboard_dir = ''\n",
    "    evaluation = False\n",
    "    snapshots = True\n",
    "    snapshot_path = \"saved/\"\n",
    "    backbone = 'resnet50'\n",
    "    epochs = 20\n",
    "    steps = training_len//(batch_size)\n",
    "    weighted_average = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "e4551ed97d6ee33f91283f559bbb7d9549d2d1f6"
   },
   "outputs": [],
   "source": [
    "train_gen,valid_gen = create_generators(args,b.preprocess_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "48482aa28034ede1fc5dd69661bbf83163ddb7bd"
   },
   "source": [
    "# Image Augmentation\n",
    "\n",
    "In addition to augmentations already done by keras-retinanet [here](https://github.com/fizyr/keras-retinanet/blob/master/keras_retinanet/bin/train.py#L227) , we'll use a package called imgaug to furthur augment the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "c09f817a02a3c385181a216f1c7c1af993efcf09"
   },
   "outputs": [],
   "source": [
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "# Define our sequence of augmentation steps that will be applied to every image.\n",
    "seq = iaa.Sequential(\n",
    "    [\n",
    "        #\n",
    "        # Execute 1 to 9 of the following (less important) augmenters per\n",
    "        # image. Don't execute all of them, as that would often be way too\n",
    "        # strong.\n",
    "        #\n",
    "        iaa.SomeOf((1, 9),\n",
    "            [\n",
    "\n",
    "                        # Blur each image with varying strength using\n",
    "                        # gaussian blur (sigma between 0 and .5),\n",
    "                        # average/uniform blur (kernel size 1x1)\n",
    "                        # median blur (kernel size 1x1).\n",
    "                        iaa.OneOf([\n",
    "                            iaa.GaussianBlur((0,0.5)),\n",
    "                            iaa.AverageBlur(k=(1)),\n",
    "                            iaa.MedianBlur(k=(1)),\n",
    "                        ]),\n",
    "\n",
    "                        # Sharpen each image, overlay the result with the original\n",
    "                        # image using an alpha between 0 (no sharpening) and 1\n",
    "                        # (full sharpening effect).\n",
    "                        iaa.Sharpen(alpha=(0, 0.25), lightness=(0.75, 1.5)),\n",
    "\n",
    "                        # Add gaussian noise to some images.\n",
    "                        # In 50% of these cases, the noise is randomly sampled per\n",
    "                        # channel and pixel.\n",
    "                        # In the other 50% of all cases it is sampled once per\n",
    "                        # pixel (i.e. brightness change).\n",
    "                        iaa.AdditiveGaussianNoise(\n",
    "                            loc=0, scale=(0.0, 0.01*255), per_channel=0.5\n",
    "                        ),\n",
    "\n",
    "                        # Either drop randomly 1 to 10% of all pixels (i.e. set\n",
    "                        # them to black) or drop them on an image with 2-5% percent\n",
    "                        # of the original size, leading to large dropped\n",
    "                        # rectangles.\n",
    "                        iaa.OneOf([\n",
    "                            iaa.Dropout((0.01, 0.1), per_channel=0.5),\n",
    "                            iaa.CoarseDropout(\n",
    "                                (0.03, 0.15), size_percent=(0.02, 0.05),\n",
    "                                per_channel=0.2\n",
    "                            ),\n",
    "                        ]),\n",
    "\n",
    "                        # Add a value of -5 to 5 to each pixel.\n",
    "                        iaa.Add((-5, 5), per_channel=0.5),\n",
    "\n",
    "                        # Change brightness of images (85-115% of original value).\n",
    "                        iaa.Multiply((0.85, 1.15), per_channel=0.5),\n",
    "\n",
    "                        # Improve or worsen the contrast of images.\n",
    "                        iaa.ContrastNormalization((0.75, 1.25), per_channel=0.5),\n",
    "\n",
    "                        # Convert each image to grayscale and then overlay the\n",
    "                        # result with the original with random alpha. I.e. remove\n",
    "                        # colors with varying strengths.\n",
    "                        iaa.Grayscale(alpha=(0.0, 0.25)),\n",
    "\n",
    "                        # In some images distort local areas with varying strength.\n",
    "                        sometimes(iaa.PiecewiseAffine(scale=(0.001, 0.01)))\n",
    "                    ],\n",
    "            # do all of the above augmentations in random order\n",
    "            random_order=True\n",
    "        )\n",
    "    ],\n",
    "    # do all of the above augmentations in random order\n",
    "    random_order=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "e2743e4a77f3b2d2e1c702b0c968e07e7603d2cc"
   },
   "outputs": [],
   "source": [
    "def augment_train_gen(train_gen,visualize=False):\n",
    "    '''\n",
    "    Creates a generator using another generator with applied image augmentation.\n",
    "    Args\n",
    "        train_gen  : keras-retinanet generator object.\n",
    "        visualize  : Boolean; False will convert bounding boxes to their anchor box targets for the model.\n",
    "    '''\n",
    "    imgs = []\n",
    "    boxes = []\n",
    "    targets = []\n",
    "    size = train_gen.size()\n",
    "    idx = 0\n",
    "    while True:\n",
    "        while len(imgs) < args.batch_size:\n",
    "            image       = train_gen.load_image(idx % size)\n",
    "            annotations = train_gen.load_annotations(idx % size)\n",
    "            image,annotations = train_gen.random_transform_group_entry(image,annotations)\n",
    "            imgs.append(image)            \n",
    "            boxes.append(annotations['bboxes'])\n",
    "            targets.append(annotations)\n",
    "            idx += 1\n",
    "        if visualize:\n",
    "            imgs = seq.augment_images(imgs)\n",
    "            im2 = np.array(imgs)\n",
    "            boxes = np.array(boxes)\n",
    "            yield im2,boxes\n",
    "        else:\n",
    "            imgs = seq.augment_images(imgs)\n",
    "            imgs,targets = train_gen.preprocess_group(imgs,targets)\n",
    "            imgs = train_gen.compute_inputs(imgs)\n",
    "            targets = train_gen.compute_targets(imgs,targets)\n",
    "            im2 = np.array(imgs)\n",
    "            yield im2,targets\n",
    "        imgs = []\n",
    "        boxes = []\n",
    "        targets = []\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "755f6bb05e3251d40877ddba74c8f6486678f47c"
   },
   "source": [
    "# Visualize augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bcd7d99112b845fe8b42ca980d3842361db93614"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "skip_batches = 5\n",
    "i = 0\n",
    "\n",
    "for imgs,boxes in augment_train_gen(train_gen,visualize=True):\n",
    "    if i > skip_batches:\n",
    "        fig=plt.figure(figsize=(24,96))\n",
    "        columns = 2\n",
    "        rows = 8\n",
    "        for i in range(1, columns*rows + 1):\n",
    "            draw_boxes(imgs[i], boxes[i], (0, 255, 0), thickness=1)\n",
    "            fig.add_subplot(rows, columns, i)\n",
    "            plt.imshow(cv2.cvtColor(imgs[i],cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        break\n",
    "    else:\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "35f45954269581be05dbc6a9601965db625609d8"
   },
   "source": [
    "# More Hyperparameters\n",
    "\n",
    "we'll use learning rate of 0.001 and freeze weights for the backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "de392d655a630d3b73dfa84fe1febbed0f3fdbe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /data/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model, training_model, prediction_model = create_models(\n",
    "            backbone_retinanet=b.retinanet,\n",
    "            num_classes=train_gen.num_classes(),\n",
    "            weights=None,\n",
    "            multi_gpu=False,\n",
    "            freeze_backbone=True,\n",
    "            lr=1e-3,\n",
    "            config=args.config\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger(filename=\"training_history.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "a34e90ba8fec837af9007610241b487b9a6a807c"
   },
   "outputs": [],
   "source": [
    "callbacks = create_callbacks(\n",
    "    model,\n",
    "    training_model,\n",
    "    prediction_model,\n",
    "    valid_gen,\n",
    "    args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "33285a3b74dc8580797f86678e080cbfecd7c668"
   },
   "source": [
    "# Download pretrained model\n",
    "\n",
    "We download a pretrained model on COCO dataset and load it's weights, we'll skip loading the weights for the few last layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "764a0430cc7c5f5f341bb3ba3bb89b699248481e"
   },
   "outputs": [],
   "source": [
    "#!wget https://github.com/fizyr/keras-retinanet/releases/download/0.5.1/resnet50_coco_best_v2.1.0.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "275ddf407b82092a0ec8a514500c0987f918d59b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer regression_submodel due to mismatch in shape ((3, 3, 256, 120) vs (36, 256, 3, 3)).\n",
      "  weight_values[i].shape))\n",
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer regression_submodel due to mismatch in shape ((120,) vs (36,)).\n",
      "  weight_values[i].shape))\n",
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer classification_submodel due to mismatch in shape ((3, 3, 256, 30) vs (720, 256, 3, 3)).\n",
      "  weight_values[i].shape))\n",
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/keras/engine/saving.py:1140: UserWarning: Skipping loading of weights for layer classification_submodel due to mismatch in shape ((30,) vs (720,)).\n",
      "  weight_values[i].shape))\n"
     ]
    }
   ],
   "source": [
    "training_model.load_weights('resnet50_coco_best_v2.1.0.h5',skip_mismatch=True,by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_model.load_weights('saved/firstrun_19.h5',skip_mismatch=True,by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f7cd58d21715eb0ec3a9de5b30be8cea85915726"
   },
   "source": [
    "# Train the model\n",
    "\n",
    "We will train for 70 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "06bca3482f679c50640b7396bafb9b291b4f402b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "34/84 [===========>..................] - ETA: 21:16 - loss: 2.7510 - regression_loss: 2.0069 - classification_loss: 0.7441"
     ]
    }
   ],
   "source": [
    "training_model.fit_generator(generator=augment_train_gen(train_gen),\n",
    "        steps_per_epoch=args.steps,\n",
    "        epochs=args.epochs,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks + [csv_logger],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a5b84c37c58444c37e6d08fb6de19b756c03926"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
